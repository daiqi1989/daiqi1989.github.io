---
---

@string{aps = {American Physical Society,}}

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM Multimedia})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(ICML  = {ICML})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})
@String(ACL = {ACL})


@article{tian2024reducio,
  title={REDUCIO! Generating 1024\(\times\)1024 Video within 16 Seconds using Extremely Compressed Motion Latents},
  author={Tian, Rui and Dai, Qi and Bao, Jianmin and Qiu, Kai and Yang, Yifan and Luo, Chong and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2411.13552},
  arxiv={2411.13552},
  code={https://github.com/microsoft/Reducio-VAE},
  year={2024}
}

@article{huang2024llm2clip,
  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},
  author={Huang, Weiquan and Wu, Aoqi and Yang, Yifan and Luo, Xufang and Yang, Yuqing and Hu, Liang and Dai, Qi and Dai, Xiyang and Chen, Dongdong and Luo, Chong and Qiu, Lili},
  journal={arXiv preprint arXiv:2411.04997},
  arxiv={2411.04997},
  website={https://microsoft.github.io/LLM2CLIP},
  code={https://github.com/microsoft/LLM2CLIP},
  year={2024}
}

@inproceedings{zhang2024aligning,
  title={Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms},
  author={Zhang, Miaosen and Wei, Yixuan and Xing, Zhen and Ma, Yifei and Wu, Zuxuan and Li, Ji and Zhang, Zheng and Dai, Qi and Luo, Chong and Geng, Xin and Guo, Baining},
  booktitle={NeurIPS},
  pdf={Aligning_Vision_Models.pdf},
  year={2024}
}

@inproceedings{li2024human,
  title={Human-aware vision-and-language navigation: Bridging simulation to reality with dynamic human interactions},
  author={Li, Heng and Li, Minghan and Cheng, Zhi-Qi and Dong, Yifei and Zhou, Yuxuan and He, Jun-Yan and Dai, Qi and Mitamura, Teruko and Hauptmann, Alexander G},
  booktitle={NeurIPS},
  pdf={HAVLN.pdf},
  website={https://lpercc.github.io/HA3D_simulator/},
  code={https://github.com/lpercc/HA3D_simulator},
  year={2024}
}

@article{tu2024motionfollower,
  title={MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion},
  author={Tu, Shuyuan and Dai, Qi and Zhang, Zihao and Xie, Sicheng and Cheng, Zhi-Qi and Luo, Chong and Han, Xintong and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2405.20325},
  arxiv={2405.20325},
  website={https://francis-rings.github.io/MotionFollower/},
  year={2024}
}

@inproceedings{wang2024microcinema,
  title={MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation},
  author={Wang, Yanhui and Bao, Jianmin and Weng, Wenming and Feng, Ruoyu and Yin, Dacheng and Yang, Tao and Zhang, Jingxu and Dai, Qi and Zhao, Zhiyuan and Wang, Chunyu and Qiu, Kai and Yuan, Yuhui and Sun, Xiaoyan and Luo, Chong and Guo, Baining},
  booktitle={CVPR},
  pdf={Wang_MicroCinema_A_Divide-and-Conquer_Approach_for_Text-to-Video_Generation_CVPR_2024_paper.pdf},
  supp={Wang_MicroCinema_A_Divide-and-Conquer_CVPR_2024_supplemental.pdf},
  website={https://wangyanhui666.github.io/MicroCinema.github.io/},
  year={2024}
}

@inproceedings{tu2024motioneditor,
  title={MotionEditor: Editing Video Motion via Content-Aware Diffusion},
  author={Tu, Shuyuan and Dai, Qi and Cheng, Zhi-Qi and Hu, Han and Han, Xintong and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  pdf={Tu_MotionEditor_Editing_Video_Motion_via_Content-Aware_Diffusion_CVPR_2024_paper.pdf},
  supp={Tu_MotionEditor_Editing_Video_CVPR_2024_supplemental.pdf},
  website={https://francis-rings.github.io/MotionEditor/},
  code={https://github.com/Francis-Rings/MotionEditor},
  year={2024}
}

@inproceedings{xing2024simda,
  title={SimDA: Simple Diffusion Adapter for Efficient Video Generation},
  author={Xing, Zhen and Dai, Qi and Hu, Han and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  pdf={Xing_SimDA_Simple_Diffusion_Adapter_for_Efficient_Video_Generation_CVPR_2024_paper.pdf},
  supp={Xing_SimDA_Simple_Diffusion_CVPR_2024_supplemental.pdf},
  website={https://chenhsing.github.io/SimDA/},
  code={https://github.com/ChenHsing/SimDA},
  year={2024}
}

@inproceedings{Zhou_2024_CVPR,
    author    = {Zhou, Yuxuan and Yan, Xudong and Cheng, Zhi-Qi and Yan, Yan and Dai, Qi and Hua, Xian-Sheng},
    title     = {BlockGCN: Redefine Topology Awareness for Skeleton-Based Action Recognition},
    booktitle = {CVPR},
    pdf       = {Zhou_BlockGCN_Redefine_Topology_Awareness_for_Skeleton-Based_Action_Recognition_CVPR_2024_paper.pdf},
    supp      = {Zhou_BlockGCN_Redefine_Topology_CVPR_2024_supplemental.pdf},
    code      = {https://github.com/ZhouYuxuanYX/BlockGCN},
    year      = {2024}
}

@inproceedings{weng2024artv,
  title={ARTV: Auto-Regressive Text-to-Video Generation with Diffusion Models},
  author={Weng, Wenming and Feng, Ruoyu and Wang, Yanhui and Dai, Qi and Wang Chunyu and Yin, Dacheng and Zhao, Zhiyuan and Qiu, Kai and Bao, Jianmin and Yuan, Yuhui and Luo, Chong and Zhang, Yueyi and Xiong, Zhiwei},
  booktitle={CVPRW},
  pdf={ART-V.pdf},
  supp={ART-V_supplemental.pdf},
  website={https://warranweng.github.io/art.v/},
  year={2024}
}

@article{xing2024survey,
  title={A survey on video diffusion models},
  author={Xing, Zhen and Feng, Qijun and Chen, Haoran and Dai, Qi and Hu, Han and Xu, Hang and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={ACM Computing Surveys},
  pdf={vdmsurvey.pdf},
  website={https://github.com/ChenHsing/Awesome-Video-Diffusion-Models},
  year={2024}
}

@article{tian2024role,
  title={The Role of ViT Design and Training in Robustness Towards Common Corruptions},
  author={Tian, Rui and Wu, Zuxuan and Dai, Qi and Goldblum, Micah and Hu, Han and Jiang, Yu-Gang},
  journal={IEEE Transactions on Multimedia},
  pdf={TMM_final_Robustness.pdf},
  year={2024}
}

@inproceedings{xing2023svformer,
  title={SVFormer: Semi-supervised Video Transformer for Action Recognition},
  author={Xing, Zhen and Dai, Qi and Hu, Han and Chen, Jingjing and Wu, Zuxuan and Jiang, Yu-Gang},
  booktitle={CVPR},
  pdf={SVFormer.pdf},
  code={https://github.com/ChenHsing/SVFormer},
  pages={18816--18826},
  year={2023}
}

@inproceedings{tian2023resformer,
  title={ResFormer: Scaling ViTs with Multi-Resolution Training},
  author={Tian, Rui and Wu, Zuxuan and Dai, Qi and Hu, Han and Qiao, Yu and Jiang, Yu-Gang},
  booktitle={CVPR},
  pdf={ResFormer.pdf},
  pages={22721--22731},
  year={2023}
}

@inproceedings{xie2023data,
  title={On Data Scaling in Masked Image Modeling},
  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Wei, Yixuan and Dai, Qi and Hu, Han},
  booktitle={CVPR},
  pdf={DataScaling.pdf},
  supp={Xie_On_Data_Scaling_CVPR_2023_supplemental.pdf},
  pages={10365--10374},
  year={2023}
}

@inproceedings{zhang2022hivit,
  title={HiViT: A simpler and more efficient design of hierarchical vision transformer},
  author={Zhang, Xiaosong and Tian, Yunjie and Xie, Lingxi and Huang, Wei and Dai, Qi and Ye, Qixiang and Tian, Qi},
  booktitle={ICLR},
  pdf={hivit.pdf},
  code={https://github.com/zhangxiaosong18/hivit},
  year={2023}
}

@inproceedings{tu2023implicit,
  title={Implicit Temporal Modeling with Learnable Alignment for Video Recognition},
  author={Tu, Shuyuan and Dai, Qi and Wu, Zuxuan and Cheng, Zhi-Qi and Hu, Han and Jiang, Yu-Gang},
  booktitle={ICCV},
  pdf={ILA.pdf},
  supp={Tu_Implicit_Temporal_Modeling_ICCV_2023_supplemental.pdf},
  slides={ILA_slides.pdf},
  code={https://github.com/Francis-Rings/ILA},
  pages={19936--19947},
  year={2023}
}

@inproceedings{ning2023all,
  title={All in Tokens: Unifying Output Space of Visual Tasks via Soft Token},
  author={Ning, Jia and Li, Chen and Zhang, Zheng and Wang, Chunyu and Geng, Zigang and Dai, Qi and He, Kun and Hu, Han},
  booktitle={ICCV},
  pdf={AiT.pdf},
  code={https://github.com/SwinTransformer/AiT},
  pages={19900--19910},
  year={2023}
}

@inproceedings{cheng2023chartreader,
  title={ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules},
  author={Cheng, Zhi-Qi and Dai, Qi and Hauptmann, Alexander G},
  booktitle={ICCV},
  pdf={ChartReader.pdf},
  code={https://github.com/zhiqic/ChartReader},
  pages={22202--22213},
  year={2023}
}

@article{xing2023vidiff,
  title={VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models},
  author={Xing, Zhen and Dai, Qi and Zhang, Zihao and Zhang, Hui and Hu, Han and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2311.18837},
  pdf={vidiff.pdf},
  website={https://chenhsing.github.io/VIDiff/},
  year={2023}
}

@inproceedings{liu2023parallel,
  title={Parallel sentence-level explanation generation for real-world low-resource scenarios},
  author={Liu, Yan and Chen, Xiaokang and Dai, Qi},
  booktitle={ICASSP},
  pdf={cnat.pdf},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{wu2023deep,
  title={Deep Uncoupled Discrete Hashing via Similarity Matrix Decomposition},
  author={Wu, Dayan and Dai, Qi and Li, Bo and Wang, Weiping},
  journal={ACM TOMM},
  pdf={DUDH.pdf},
  volume={19},
  number={1},
  pages={1--22},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{xie2022simmim,
  title={SimMIM: A Simple Framework for Masked Image Modeling},
  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
  booktitle={CVPR},
  pdf={SimMIM.pdf},
  supp={Xie_SimMIM_A_Simple_CVPR_2022_supplemental.pdf},
  code={https://github.com/microsoft/SimMIM},
  pages={9653--9663},
  year={2022}
}

@inproceedings{cheng2022rethinking,
  title={Rethinking Spatial Invariance of Convolutional Networks for Object counting},
  author={Cheng, Zhi-Qi and Dai, Qi and Li, Hong and Song, Jingkuan and Wu, Xiao and Hauptmann, Alexander G},
  booktitle={CVPR},
  pdf={rethinking.pdf},
  code={https://github.com/zhiqic/Rethinking-Counting},
  pages={19638--19648},
  year={2022}
}

@article{han2021connection,
  title={On the Connection between Local Attention and Dynamic Depth-Wise Convolution},
  author={Han, Qi and Fan, Zejia and Dai, Qi and Sun, Lei and Cheng, Ming-Ming and Liu, Jiaying and Wang, Jingdong},
  journal={ICLR},
  pdf={connection.pdf},
  slides={connection_slides.pdf},
  code={https://github.com/Atten4Vis/DemystifyLocalViT},
  year={2022}
}

@inproceedings{cheng2022gsrformer,
  title={GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement},
  author={Cheng, Zhi-Qi and Dai, Qi and Li, Siyao and Mitamura, Teruko and Hauptmann, Alexander},
  booktitle={ACM Multimedia},
  pdf={GSRFormer.pdf},
  code={https://github.com/zhiqic/GSRFormer},
  pages={3272--3281},
  year={2022}
}

@inproceedings{liu2022mpii,
  title={MPII: Multi-level Mutual Promotion for Inference and Interpretation},
  author={Liu, Yan and Chen, Sanyuan and Yang, Yazheng and Dai, Qi},
  booktitle={ACL},
  pdf={MPII.pdf},
  code={https://github.com/theNamek/MPII},
  pages={7074--7084},
  year={2022}
}

@inproceedings{shi2021temporal,
  title={Temporal Action Detection with Multi-Level Supervision},
  author={Shi, Baifeng and Dai, Qi and Hoffman, Judy and Saenko, Kate and Darrell, Trevor and Xu, Huijuan},
  booktitle={ICCV},
  pdf={tad_multi},
  pages={8022--8032},
  year={2021}
}

@article{xie2021self,
  title={Self-Supervised Learning with Swin Transformers},
  author={Xie, Zhenda and Lin, Yutong and Yao, Zhuliang and Zhang, Zheng and Dai, Qi and Cao, Yue and Hu, Han},
  pdf={MOBY.pdf},
  code={https://github.com/SwinTransformer/Transformer-SSL},
  journal={arXiv preprint arXiv:2105.04553},
  year={2021}
}

@article{min2021cross,
  title={Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning},
  author={Min, Shaobo and Dai, Qi and Xie, Hongtao and Gan, Chuang and Zhang, Yongdong and Wang, Jingdong},
  pdf={CMAC.pdf},
  journal={arXiv preprint arXiv:2106.06939},
  year={2021}
}

@article{he2021novel,
  title={A Novel Class Restriction Loss for Unsupervised Domain Adaptation},
  author={He, Qi and Dai, Qi and Wu, Xiao and He, Jun-Yan},
  journal={Neurocomputing},
  pdf={neurocomp_CRL},
  volume={461},
  pages={254--265},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{shi2020informative,
  title={Informative Dropout for Robust Representation Learning: A Shape-bias Perspective},
  author={Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
  booktitle={ICML},
  pdf={InfoDrop.pdf},
  code={https://github.com/bfshi/InfoDrop},
  pages={8828--8839},
  year={2020},
  organization={PMLR}
}

@inproceedings{shi2020weakly,
  title={Weakly-Supervised Action Localization by Generative Attention Modeling},
  author={Shi, Baifeng and Dai, Qi and Mu, Yadong and Wang, Jingdong},
  booktitle={CVPR},
  pdf={DGAM.pdf},
  code={https://github.com/bfshi/DGAM-Weakly-Supervised-Action-Localization},
  pages={1009--1019},
  year={2020}
}

@article{liu2020reinforced,
  title={Reinforced Short-length Hashing},
  author={Liu, Xingbo and Nie, Xiushan and Dai, Qi and Huang, Yupan and Lian, Li and Yin, Yilong},
  journal={IEEE TCSVT},
  pdf={RSLH.pdf},
  volume={31},
  number={9},
  pages={3655--3668},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wu2019deep,
  title={Deep Incremental Hashing Network for Efficient Image Retrieval},
  author={Wu, Dayan and Dai, Qi and Liu, Jing and Li, Bo and Wang, Weiping},
  booktitle={CVPR},
  pdf={DIHN.pdf},
  code={https://github.com/IIE-MR/DIHN},
  pages={9069--9077},
  year={2019}
}

@inproceedings{cheng2019learning,
  title={Learning Spatial Awareness to Improve Crowd Counting},
  author={Cheng, Zhi-Qi and Li, Jun-Xiu and Dai, Qi and Wu, Xiao and Hauptmann, Alexander G},
  booktitle={ICCV},
  pdf={SPANet.pdf},
  pages={6152--6161},
  year={2019}
}

@inproceedings{cheng2019improving,
  title={Improving the Learning of Multi-Column Convolutional Neural Network for Crowd Counting},
  author={Cheng, Zhi-Qi and Li, Jun-Xiu and Dai, Qi and Wu, Xiao and He, Jun-Yan and Hauptmann, Alexander G},
  booktitle={ACM Multimedia},
  pdf={McML.pdf},
  pages={1897--1906},
  year={2019}
}

@inproceedings{huang2019decoupling,
  title={Decoupling Localization and Classification in Single Shot Temporal Action Detection},
  author={Huang, Yupan and Dai, Qi and Lu, Yutong},
  booktitle={ICME},
  pdf={Decoupling.pdf},
  code={https://github.com/hypjudy/Decouple-SSAD},
  pages={1288--1293},
  year={2019},
  organization={IEEE}
}

@inproceedings{li2018recurrent,
  title={Recurrent Tubelet Proposal and Recognition Networks for Action Detection},
  author={Li, Dong and Qiu, Zhaofan and Dai, Qi and Yao, Ting and Mei, Tao},
  booktitle={ECCV},
  pdf={RTPR.pdf},
  pages={303--318},
  year={2018}
}

@inproceedings{long2018deep,
  title={Deep Domain Adaptation Hashing with Adversarial Learning},
  author={Long, Fuchen and Yao, Ting and Dai, Qi and Tian, Xinmei and Luo, Jiebo and Mei, Tao},
  booktitle={SIGIR},
  pdf={DeDAHA.pdf},
  pages={725--734},
  year={2018}
}

@inproceedings{dai2016binary,
  title={Binary Optimized Hashing},
  author={Dai, Qi and Li, Jianguo and Wang, Jingdong and Jiang, Yu-Gang},
  booktitle={ACM Multimedia},
  pdf={BOH_Dai.pdf},
  pages={1247--1256},
  year={2016}
}

@article{dai2016bayesian,
  title={A Bayesian Hashing Approach and its Application to Face Recognition},
  author={Dai, Qi and Li, Jianguo and Wang, Jun and Chen, Yurong and Jiang, Yu-Gang},
  journal={Neurocomputing},
  pdf={BayesianHashing_Journal_Print_version.pdf},
  volume={213},
  pages={5--13},
  year={2016},
  publisher={Elsevier}
}


@inproceedings{dai2015optimal,
  title={Optimal Bayesian Hashing for Efficient Face Recognition},
  author={Dai, Qi and Li, Jianguo and Wang, Jun and Chen, Yurong and Jiang, Yu-Gang},
  booktitle={IJCAI},
  pdf={bayesian.pdf},
  year={2015}
}

@article{jiang2015human,
  title={Human Action Recognition in Unconstrained Videos by Explicit Motion Modeling},
  author={Jiang, Yu-Gang and Dai, Qi and Liu, Wei and Xue, Xiangyang and Ngo, Chong-Wah},
  journal={IEEE TIP},
  pdf={Action_Recognition_Unconstrained.pdf},
  volume={24},
  number={11},
  pages={3781--3795},
  year={2015},
  publisher={IEEE}
}

@article{jiang2015super,
  title={Super Fast Event Recognition in Internet Videos},
  author={Jiang, Yu-Gang and Dai, Qi and Mei, Tao and Rui, Yong and Chang, Shih-Fu},
  journal={IEEE TMM},
  pdf={super.pdf},
  volume={17},
  number={8},
  pages={1174--1186},
  year={2015},
  publisher={IEEE}
}

@inproceedings{dai2015fudan,
  title={Fudan-Huawei at MediaEval 2015: Detecting Violent Scenes and Affective Impact in Movies with Deep Learning},
  author={Dai, Qi and Zhao, Rui-Wei and Wu, Zuxuan and Wang, Xi and Gu, Zichen and Wu, Wenhai and Jiang, Yu-Gang},
  booktitle={MediaEval},
  pdf={MediaEval2015-Fudan-Huawei.pdf},
  volume={1436},
  year={2015}
}

@inproceedings{dai2014fudan,
  title={Fudan-NJUST at MediaEval 2014: Violent Scenes Detection Using Deep Neural Networks},
  author={Dai, Qi and Wu, Zuxuan and Jiang, Yu-Gang and Xue, Xiangyang and Tang, Jinhui},
  booktitle={MediaEval},
  pdf={MediaEval2014-Fudan-NJUST.pdf},
  year={2014}
}

@inproceedings{tu2014challenge,
  title={Challenge Huawei challenge: Fusing Multimodal Features with Deep Neural Networks for Mobile Video Annotation},
  author={Tu, Jian and Wu, Zuxuan and Dai, Qi and Jiang, Yu-Gang and Xue, Xiangyang},
  booktitle={ICMEW},
  pdf={icme_challenge.pdf},
  pages={1--6},
  year={2014},
  organization={IEEE}
}

@inproceedings{wang2013beauty,
  title={Beauty is here: Evaluating Aesthetics in Videos using Multimodal Features and Free Training Data},
  author={Wang, Yanran and Dai, Qi and Feng, Rui and Jiang, Yu-Gang},
  booktitle={ACM Multimedia},
  pdf={ACMM2013.pdf},
  pages={369--372},
  year={2013}
}

@inproceedings{dai2013fudan,
  title={Fudan at MediaEval 2013: Violent Scenes Detection Using Motion Features and Part-Level Attributes},
  author={Dai, Qi and Tu, Jian and Shi, Ziqiang and Jiang, Yu-Gang and Xue, Xiangyang},
  booktitle={MediaEval},
  pdf={MediaEval2013.pdf},
  year={2013}
}

@inproceedings{jiang2012trajectory,
  title={Trajectory-based Modeling of Human Actions with Motion Reference Points},
  author={Jiang, Yu-Gang and Dai, Qi and Xue, Xiangyang and Liu, Wei and Ngo, Chong-Wah},
  booktitle={ECCV},
  pdf={Traj.pdf},
  pages={425--438},
  year={2012},
  organization={Springer}
}

@article{jiang2012fastsemantic,
  title={Fast Semantic Diffusion for Large-scale Context-based Image and Video Annotation},
  author={Jiang, Yu-Gang and Dai, Qi and Wang, Jun and Ngo, Chong-Wah and Xue, Xiangyang and Chang, Shih-Fu},
  journal={IEEE TIP},
  pdf={TIPContext.pdf},
  volume={21},
  number={6},
  pages={3080--3091},
  year={2012},
  publisher={IEEE}
}

@inproceedings{jiang2012fast,
  title={A Fast Video Event Recognition System and its Application to Video Search},
  author={Jiang, Yu-Gang and Dai, Qi and Zheng, Yingbin and Xue, Xiangyang and Liu, Jie and Wang, Dong},
  booktitle={ACM Multimedia (Demo)},
  pdf={fast_retrieval_system.pdf},
  pages={1347--1348},
  year={2012}
}

@inproceedings{jiang2012shanghai,
  title={The Shanghai-Hongkong team at MediaEval2012: Violent Scene Detection using Trajectory-based Features},
  author={Jiang, Yu-Gang and Dai, Qi and Tan, Chun Chet and Xue, Xiangyang and Ngo, Chong-Wah},
  booktitle={MediaEval},
  pdf={MediaEval2012.pdf},
  year={2012},
  publisher={SpringerLink}
}






